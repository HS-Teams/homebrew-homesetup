| Pull Name                     | Model Family                | Params / Variant | Download Size | Context Window | Capabilities                     | Description                                                                                                                                                                            |
|:------------------------------|:----------------------------|:-----------------|:--------------|:---------------|:---------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `aya-expanse:32b`             | aya-expanse                 | 32B              | 20GB          | 8K             | Text, Multilingual               | Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages.                                                                        |
| `aya-expanse:latest`          | aya-expanse                 | 8B               | 5.1GB         | 8K             | Text, Multilingual               | Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages.                                                                        |
| `codegemma:2b`                | codegemma                   | 2B               | 1.6GB         | 8K             | Text, Code, Fill-in-middle       | CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks...                                                                                |
| `codegemma:latest`            | codegemma                   | 7B               | 5.0GB         | 8K             | Text, Code, Fill-in-middle       | CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks...                                                                                |
| `codellama:13b`               | codellama                   | 13B              | 7.4GB         | 16K            | Text, Code, Fill-in-middle       | A large language model that can use text prompts to generate and discuss code.                                                                                                         |
| `codellama:34b`               | codellama                   | 34B              | 19GB          | 16K            | Text, Code, Fill-in-middle       | A large language model that can use text prompts to generate and discuss code.                                                                                                         |
| `codellama:70b`               | codellama                   | 70B              | 39GB          | 2K             | Text, Code, Fill-in-middle       | A large language model that can use text prompts to generate and discuss code.                                                                                                         |
| `codellama:latest`            | codellama                   | 7B               | 3.8GB         | 16K            | Text, Code, Fill-in-middle       | A large language model that can use text prompts to generate and discuss code.                                                                                                         |
| `command-r:latest`            | command-r                   | 35B              | 19GB          | 128K           | Text, RAG, Tool Use              | Command R is a Large Language Model optimized for conversational interaction and long context tasks.                                                                                   |
| `deepseek-coder-v2:236b`      | deepseek-coder-v2           | 236B             | 133GB         | 4K             | Text, Code                       | An open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo.                                                                        |
| `deepseek-coder-v2:latest`    | deepseek-coder-v2           | 16B              | 8.9GB         | 160K           | Text, Code                       | An open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo.                                                                        |
| `deepseek-r1:1.5b`            | deepseek-r1                 | 1.5B             | 1.1GB         | 128K           | Text, Reasoning, Code            | DeepSeek-R1 is a family of open reasoning models with performance approaching that of leading models, such as O3 and Gemini 2.5 Pro.                                                   |
| `deepseek-r1:70b`             | deepseek-r1                 | 70B              | 43GB          | 128K           | Text, Reasoning, Code            | DeepSeek-R1 is a family of open reasoning models with performance approaching that of leading models, such as O3 and Gemini 2.5 Pro.                                                   |
| `deepseek-r1:latest`          | deepseek-r1                 | 8B               | 5.2GB         | 128K           | Text, Reasoning, Code            | DeepSeek-R1 is a family of open reasoning models with performance approaching that of leading models, such as O3 and Gemini 2.5 Pro.                                                   |
| `deepseek-v3.1:latest`        | deepseek-v3.1               | 671B             | 404GB         | 160K           | Text, Tools, Thinking            | DeepSeek-V3.1-Terminus is a hybrid model that supports both thinking mode and non-thinking mode.                                                                                       |
| `embeddinggemma:latest`       | embeddinggemma              | 300M             | 622MB         | 2K             | Embedding                        | EmbeddingGemma is a 300M parameter embedding model from Google.                                                                                                                        |
| `falcon3:10b`                 | falcon3                     | 10B              | 6.3GB         | 32K            | Text, Science, Math, Code        | A family of efficient AI models under 10B parameters performant in science, math, and coding...                                                                                        |
| `falcon3:1b`                  | falcon3                     | 1B               | 1.8GB         | 8K             | Text, Science, Math, Code        | A family of efficient AI models under 10B parameters performant in science, math, and coding...                                                                                        |
| `falcon3:latest`              | falcon3                     | 7B               | 4.6GB         | 32K            | Text, Science, Math, Code        | A family of efficient AI models under 10B parameters performant in science, math, and coding...                                                                                        |
| `falcon:180b`                 | falcon                      | 180B             | 101GB         | 2K             | Text, Chat, Summarization        | A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and chat bots.                                                    |
| `falcon:40b`                  | falcon                      | 40B              | 24GB          | 2K             | Text, Chat, Summarization        | A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and chat bots.                                                    |
| `falcon:latest`               | falcon                      | 7B               | 4.2GB         | 2K             | Text, Chat, Summarization        | A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and chat bots.                                                    |
| `gemma2:27b`                  | gemma2                      | 27B              | 16GB          | 8K             | Text, Chat, Summarization        | Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.                                                                                     |
| `gemma2:2b`                   | gemma2                      | 2B               | 1.6GB         | 8K             | Text, Chat, Summarization        | Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.                                                                                     |
| `gemma2:latest`               | gemma2                      | 9B               | 5.4GB         | 8K             | Text, Chat, Summarization        | Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.                                                                                     |
| `gemma3:12b`                  | gemma3                      | 12B              | 8.1GB         | 128K           | Text, Image, Multimodal          | The current, most capable model that runs on a single GPU.                                                                                                                             |
| `gemma3:1b`                   | gemma3                      | 1B               | 815MB         | 32K            | Text                             | The current, most capable model that runs on a single GPU.                                                                                                                             |
| `gemma3:270m`                 | gemma3                      | 270M             | 292MB         | 32K            | Text                             | The current, most capable model that runs on a single GPU.                                                                                                                             |
| `gemma3:27b`                  | gemma3                      | 27B              | 17GB          | 128K           | Text, Image, Multimodal          | The current, most capable model that runs on a single GPU.                                                                                                                             |
| `gemma3:latest`               | gemma3                      | 4B               | 3.3GB         | 128K           | Text, Image, Multimodal          | The current, most capable model that runs on a single GPU.                                                                                                                             |
| `gemma:2b`                    | gemma                       | 2B               | 1.7GB         | 8K             | Text                             | Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1.                                                                       |
| `gemma:latest`                | gemma                       | 7B               | 5.0GB         | 8K             | Text                             | Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1.                                                                       |
| `glm-4.6:cloud`               | glm-4.6                     | (Cloud)          | -             | 198K           | Text, Agentic, Reasoning, Code   | Advanced agentic, reasoning and coding capabilities.                                                                                                                                   |
| `gpt-oss:120b`                | gpt-oss                     | 120B             | 65GB          | 128K           | Text, Reasoning, Agentic Tasks   | OpenAI's open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.                                                                         |
| `gpt-oss:latest`              | gpt-oss                     | 20B              | 14GB          | 128K           | Text, Reasoning, Agentic Tasks   | OpenAI's open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.                                                                         |
| `llama2-uncensored:latest`    | llama2-uncensored           | 7B               | 3.8GB         | 2K             | Text                             | Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.                                                                                               |
| `llama2:13b`                  | llama2                      | 13B              | 7.4GB         | 4K             | Text, Chat                       | Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.                                                                                               |
| `llama2:70b`                  | llama2                      | 70B              | 39GB          | 4K             | Text, Chat                       | Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.                                                                                               |
| `llama2:latest`               | llama2                      | 7B               | 3.8GB         | 4K             | Text, Chat                       | Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.                                                                                               |
| `llama3-chatqa:70b`           | llama3-chatqa               | 70B              | 40GB          | 8K             | Text, Chat QA                    | Meta Llama 3: The most capable openly available LLM to date.                                                                                                                           |
| `llama3-chatqa:latest`        | llama3-chatqa               | 8B               | 4.7GB         | 8K             | Text, Chat QA                    | Meta Llama 3: The most capable openly available LLM to date.                                                                                                                           |
| `llama3-groq-tool-use:70b`    | llama3-groq-tool-use        | 70B              | 40GB          | 8K             | Text, Tool Use                   | Meta Llama 3: The most capable openly available LLM to date.                                                                                                                           |
| `llama3-groq-tool-use:latest` | llama3-groq-tool-use        | 8B               | 4.7GB         | 8K             | Text, Tool Use                   | Meta Llama 3: The most capable openly available LLM to date.                                                                                                                           |
| `llama3.1:405b`               | llama3.1                    | 405B             | 243GB         | 128K           | Text, Multilingual, Tool Use     | Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.                                                                                     |
| `llama3.1:70b`                | llama3.1                    | 70B              | 43GB          | 128K           | Text, Multilingual, Tool Use     | Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.                                                                                     |
| `llama3.1:latest`             | llama3.1                    | 8B               | 4.9GB         | 128K           | Text, Multilingual, Tool Use     | Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.                                                                                     |
| `llama3.2-vision:90b`         | llama3.2-vision             | 90B              | 55GB          | 128K           | Text, Image, Vision              | Meta's Llama 3.2 goes small with 1B and 3B models.                                                                                                                                     |
| `llama3.2-vision:latest`      | llama3.2-vision             | 11B              | 7.8GB         | 128K           | Text, Image, Vision              | Meta's Llama 3.2 goes small with 1B and 3B models.                                                                                                                                     |
| `llama3.2:1b`                 | llama3.2                    | 1B               | 1.3GB         | 128K           | Text, Multilingual, Tool Use     | Meta's Llama 3.2 goes small with 1B and 3B models.                                                                                                                                     |
| `llama3.2:latest`             | llama3.2                    | 3B               | 2.0GB         | 128K           | Text, Multilingual, Tool Use     | Meta's Llama 3.2 goes small with 1B and 3B models.                                                                                                                                     |
| `llama3.3:latest`             | llama3.3                    | 70B              | 43GB          | 128K           | Text, Multilingual, Tools        | New state of the art 70B model. Llama 3.3 70B offers similar performance compared to the Llama 3.1 405B model.                                                                         |
| `llama3:70b`                  | llama3                      | 70B              | 40GB          | 8K             | Text, Chat                       | Meta Llama 3: The most capable openly available LLM to date.                                                                                                                           |
| `llama3:8b`                   | llama3                      | 8B               | 4.7GB         | 8K             | Text, Chat                       | Meta Llama 3: The most capable openly available LLM to date.                                                                                                                           |
| `llama3:latest`               | llama3                      | 8B               | 4.7GB         | 8K             | Text, Chat                       | Meta Llama 3: The most capable openly available LLM to date.                                                                                                                           |
| `llava:13b`                   | llava                       | 13B              | 8.0GB         | 4K             | Text, Image, Vision, OCR         | ðŸŒ‹ LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6. |
| `llava:34b`                   | llava                       | 34B              | 20GB          | 4K             | Text, Image, Vision, OCR         | ðŸŒ‹ LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6. |
| `llava:latest`                | llava                       | 7B               | 4.7GB         | 32K            | Text, Image, Vision, OCR         | ðŸŒ‹ LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6. |
| `minicpm-v:latest`            | minicpm-v                   | 8B               | 5.5GB         | 32K            | Text, Image, Vision, OCR         | A series of multimodal LLMs (MLLMs) designed for vision-language understanding.                                                                                                        |
| `mistral-large:latest`        | mistral-large               | 123B             | 73GB          | 128K           | Text, Code, Math, Reasoning      | The 7B model released by Mistral AI, updated to version 0.3.                                                                                                                           |
| `mistral-nemo:latest`         | mistral-nemo                | 12B              | 7.1GB         | 1000K          | Text, Tools, Reasoning           | A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.                                                                               |
| `mistral-small:22b`           | mistral-small               | 22B              | 13GB          | 128K           | Text, Multilingual, Agentic      | The 7B model released by Mistral AI, updated to version 0.3.                                                                                                                           |
| `mistral-small:latest`        | mistral-small               | 24B              | 14GB          | 32K            | Text, Multilingual, Agentic      | The 7B model released by Mistral AI, updated to version 0.3.                                                                                                                           |
| `mistral-small3.1:latest`     | mistral-small3.1            | 24B              | 15GB          | 128K           | Text, Image, Multimodal          | The 7B model released by Mistral AI, updated to version 0.3.                                                                                                                           |
| `mistral:latest`              | mistral                     | 7B               | 4.4GB         | 32K            | Text, Tools, Function Calling    | The 7B model released by Mistral AI, updated to version 0.3.                                                                                                                           |
| `mistral:text`                | mistral                     | 7B               | 4.1GB         | 16K            | Text                             | The 7B model released by Mistral AI, updated to version 0.3.                                                                                                                           |
| `mixtral:8x22b`               | mixtral                     | 8x22B            | 80GB          | 64K            | Text, MoE, Multilingual          | A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes.                                                                             |
| `mixtral:latest`              | mixtral                     | 8x7B             | 26GB          | 32K            | Text, MoE, Multilingual          | A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes.                                                                             |
| `mxbai-embed-large:latest`    | mxbai-embed-large           | 335M             | 670MB         | 512            | Embedding                        | State-of-the-art large embedding model from mixedbread.ai.                                                                                                                             |
| `nomic-embed-text:latest`     | nomic-embed-text            | 137M             | 274MB         | 2K             | Embedding                        | A high-performing open embedding model with a large token context window.                                                                                                              |
| `phi3:latest`                 | phi3                        | 3.8B (Mini)      | 2.2GB         | 128K           | Text, Reasoning, Code            | Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.                                                                                 |
| `phi3:medium`                 | phi3                        | 14B (Medium)     | 7.9GB         | 128K           | Text, Reasoning, Code            | Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.                                                                                 |
| `phi3:mini`                   | phi3                        | 3.8B (Mini)      | 2.2GB         | 128K           | Text, Reasoning, Code            | Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.                                                                                 |
| `phi4-mini:latest`            | phi4-mini                   | 3.8B             | 2.5GB         | 128K           | Text, Reasoning, Math, Logic     | Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.                                                                                                                  |
| `phi4:latest`                 | phi4                        | 14B              | 9.1GB         | 16K            | Text, Reasoning, Logic           | Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.                                                                                                                  |
| `qwen2-math:1.5b`             | qwen2-math                  | 1.5B             | 935MB         | 4K             | Text, Math                       | Qwen2 is a new series of large language models from Alibaba group.                                                                                                                     |
| `qwen2-math:72b`              | qwen2-math                  | 72B              | 41GB          | 4K             | Text, Math                       | Qwen2 is a new series of large language models from Alibaba group.                                                                                                                     |
| `qwen2-math:latest`           | qwen2-math                  | 7B               | 4.4GB         | 4K             | Text, Math                       | Qwen2 is a new series of large language models from Alibaba group.                                                                                                                     |
| `qwen2.5-coder:32b`           | qwen2.5-coder               | 32B              | 20GB          | 32K            | Text, Code Generation, Reasoning | The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.                                                     |
| `qwen2.5-coder:7b`            | qwen2.5-coder               | 7B               | 4.7GB         | 32K            | Text, Tools, Code                | The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.                                                     |
| `qwen2.5-coder:latest`        | qwen2.5-coder               | 7B               | 4.7GB         | 32K            | Text, Code Generation, Reasoning | The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.                                                     |
| `qwen2.5:0.5b`                | qwen2.5                     | 0.5B             | 398MB         | 32K            | Text, Code, Math, Multilingual   | Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.       |
| `qwen2.5:72b`                 | qwen2.5                     | 72B              | 47GB          | 32K            | Text, Code, Math, Multilingual   | Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.       |
| `qwen2.5:latest`              | qwen2.5                     | 7B               | 4.7GB         | 32K            | Text, Code, Math, Multilingual   | Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.       |
| `qwen2.5vl:3b`                | qwen2.5vl                   | 3B               | 3.2GB         | 125K           | Text, Image, Vision, Agentic     | Qwen2 is a new series of large language models from Alibaba group.                                                                                                                     |
| `qwen2.5vl:72b`               | qwen2.5vl                   | 72B              | 49GB          | 125K           | Text, Image, Vision, Agentic     | Qwen2 is a new series of large language models from Alibaba group.                                                                                                                     |
| `qwen2.5vl:latest`            | qwen2.5vl                   | 7B               | 6.0GB         | 125K           | Text, Image, Vision, Agentic     | Qwen2 is a new series of large language models from Alibaba group.                                                                                                                     |
| `qwen3-coder:480b`            | qwen3-coder                 | 480B             | 290GB         | 256K           | Text, Code, Agentic Tasks        | Alibaba's performant long context models for agentic and coding tasks.                                                                                                                 |
| `qwen3-coder:latest`          | qwen3-coder                 | 30B              | 19GB          | 256K           | Text, Code, Agentic Tasks        | Alibaba's performant long context models for agentic and coding tasks.                                                                                                                 |
| `qwen3-vl:2b`                 | qwen3-vl                    | 2B               | 1.9GB         | 256K           | Text, Image, Vision, Video       | The most powerful vision-language model in the Qwen model family to date.                                                                                                              |
| `qwen3-vl:32b`                | qwen3-vl                    | 32B              | 21GB          | 256K           | Text, Image, Vision, Video       | The most powerful vision-language model in the Qwen model family to date.                                                                                                              |
| `qwen3-vl:latest`             | qwen3-vl                    | 8B               | 6.1GB         | 256K           | Text, Image, Vision, Video       | The most powerful vision-language model in the Qwen model family to date.                                                                                                              |
| `qwen3:0.6b`                  | qwen3                       | 0.6B             | 523MB         | 40K            | Text                             | Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models.                                   |
| `qwen3:32b`                   | qwen3                       | 32B              | 20GB          | 40K            | Text                             | Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models.                                   |
| `qwen3:4b`                    | qwen3                       | 4B               | 2.5GB         | 256K           | Text                             | Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models.                                   |
| `qwen3:latest`                | qwen3                       | 8B               | 5.2GB         | 4K             | Text                             | Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models.                                   |
| `qwen:0.5b`                   | qwen (1.5)                  | 0.5B             | 395MB         | 32K            | Text, Multilingual               | Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters.                                                                                  |
| `qwen:72b`                    | qwen (1.OPEN-SOURCE-MODELS) | 72B              | 41GB          | 32K            | Text, Multilingual               | Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters.                                                                                  |
| `qwen:latest`                 | qwen (1.5)                  | 4B               | 2.3GB         | 32K            | Text, Multilingual               | Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters.                                                                                  |
| `smollm2:135m`                | smollm2                     | 135M             | 271MB         | 8K             | Text, Tools                      | SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.                                                                               |
| `smollm2:360m`                | smollm2                     | 360M             | 726MB         | 8K             | Text, Tools                      | SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.                                                                               |
| `smollm2:latest`              | smollm2                     | 1.7B             | 1.8GB         | 8K             | Text, Tools                      | SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.                                                                               |
| `stable-code:latest`          | stable-code                 | 3B               | 1.6GB         | 16K            | Text, Code                       | Stable Code 3B is a coding model... on par with models such as Code Llama 7B that are 2.5x larger.                                                                                     |
| `starcoder:15b`               | starcoder                   | 15B              | 9.0GB         | 8K             | Text, Code                       | StarCoder is a code generation model trained on 80+ programming languages.                                                                                                             |
| `starcoder:1b`                | starcoder                   | 1B               | 726MB         | 8K             | Text, Code                       | StarCoder is a code generation model trained on 80+ programming languages.                                                                                                             |
| `starcoder:3b`                | starcoder                   | 3B               | 1.8GB         | 8K             | Text, Code                       | StarCoder is a code generation model trained on 80+ programming languages.                                                                                                             |
| `starcoder:7b`                | starcoder                   | 7B               | 4.3GB         | 8K             | Text, Code                       | StarCoder is a code generation model trained on 80+ programming languages.                                                                                                             |
| `starcoder:latest`            | starcoder                   | 3B               | 1.8GB         | 8K             | Text, Code                       | StarCoder is a code generation model trained on 80+ programming languages.                                                                                                             |
| `starcoder2:latest`           | starcoder2                  | 7B               | 4.0GB         | 16K            | Text, Code                       | StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes: 3B, 7B and 15B parameters.                                                        |
| `tinyllama:latest`            | tinyllama                   | 1.1B             | 638MB         | 2K             | Text, Chat                       | The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.                                                                                    |
| `vicuna:13b`                  | vicuna                      | 13B              | 7.4GB         | 4K             | Text, Chat                       | General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.                                                                                                        |
| `vicuna:33b`                  | vicuna                      | 33B              | 18GB          | 2K             | Text, Chat                       | General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.                                                                                                        |
| `vicuna:latest`               | vicuna                      | 7B               | 3.8GB         | 4K             | Text, Chat                       | General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.                                                                                                        |
